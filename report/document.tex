\documentclass{article}
\usepackage{geometry}
\usepackage{wrapfig}

\title{Assignment2}
\author{Wang Youan (3035237236)}
\date{\today}
\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}
\begin{document}
	\maketitle
	\section{Reflex Agent}
	\label{sec:Q1}
	The evaluation function I implemented is mainly based on two factors,
	\begin{enumerate}
		\item \textbf{The distance between ghosts and Pacman}. The evaluation function give higher score, while Pacman is further away from ghosts. The Manhattan Distance between ghost and Pacman is greater than 5 is regarded as a safe distance, and returns 5 point bonus each ghost. while less than 2 is a dangerous, immediately return minimum score. When ghost is eatable, this value will times -1. 
		\item \textbf{The distance between Pacman and food}. If next position contains a food, add 10 points, otherwise, minus the distance between Pacman and its nearest food 
	\end{enumerate}
	Also this function will give one bonus if the action is not STOP.\\
	Normally, this evaluation function acts very good in "testClassic". But as I don't take wall into consideration, this function does not act so well in "mediumClassic"
	\section{Minimax Agent}
	\label{sec:Q2}
	I first generate all the possible next actions that the agent can do, then use "evaluate\_function" to get the score of current action, then find the highest score, and return the action related to that score\\
	I use recurse to achieve the evaluate\_actions, which needs 3 inputs, agent\_index, depth, game\_state. This single method can act as both min-value and max-value based on current agent index.\\
	The problem of such agent is the speed of get action. In my Mac, for a "smallClassic" layout, when depth is set to 4, it usually takes about 3 seconds to do one action. Besides, as the evaluation function is not so good, often, Pacman will remains stay if both ghosts and food are a little far away from it.
	\section{Alpha-Beta Prune Agent}
	\label{sec:Q3}
	The technique I used in this question is slightly different from that in section~\ref{sec:Q2},
	\begin{enumerate}
		\item \textbf{Evaluate-action function add alpha and beta parameters}. These two new parameters are used to prune those unused leaves.
		\item \textbf{Use three functions to finish this recurse}. This makes the code much easier to read.
	\end{enumerate}
	This agent acts faster than Minimax Agent. If usually takes about 1.5 seconds to determine the action. However, in the same cases of that to Minimax Agent, this also choose to stay.
	\section{Expectimax Agent}
	\label{sec:Q4}
	It is true that usually ghost just random walks, not an adversary who makes optimal decisions and sometimes Minimax agent choose wrong action. That's why we need Expectimax Agent, which will calculate the possibility of every enemy's action and choose the action with highest expect score.\\
	The code of this agent looks much similar of that in section~\ref{sec:Q2}, the only difference is that instead of return min-value, it will return the average value of all actions.\\
	This gives "trappedClassic" about 49.2\% winning rate (compares to 0\% of Minimax Agent)
	\begin{thebibliography}{1}
		\bibitem{winning_strategy} 
		Plambeck, Thane E., and Greg Whitehead. 
		\textit{The Secrets of Notakto: Winning at X-only Tic-Tac-Toe}. 
		arXiv preprint arXiv:1301.1672. 2013 Jan 8.
	\end{thebibliography}
\end{document}