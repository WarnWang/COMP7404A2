\documentclass{article}
\usepackage{geometry}
\usepackage{wrapfig}

\title{Assignment2}
\author{Wang Youan (3035237236)}
\date{\today}
\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}
\begin{document}
	\maketitle
	\section{Reflex Agent}
	\label{sec:Q1}
	The evaluation function I implemented is mainly based on two factors,
	\begin{enumerate}
		\item \textbf{The distance between ghosts and Pacman}. The evaluation function give higher score, while Pacman is further away from ghosts. The Manhattan Distance between ghost and Pacman is greater than 5 is regarded as a safe distance, and returns 5 point bonus each ghost. while less than 2 is a dangerous, immediately return minimum score. When ghost is eatable, this value will times -1. 
		\item \textbf{The distance between Pacman and food}. Use Breadth first search (GSA) to find the nearest food. As the path cost is always one, BFS acts just the same as UCS, and can find the optimal solution. Besides, the process of generating points will produce many duplicate position which have been visited before, so I choose GSA to improve the speed. 
	\end{enumerate}
	Also this function will give one bonus if the action is not STOP.\\
	Normally, this evaluation function acts very good in both "testClassic" (average score is 561, and winning rate is 100\%) and "mediumClassic" (average score is 1503.171, 1584,winning rate is 99.8\%, 93.5\% for one ghost and two ghosts respectively).
	\section{Minimax Agent}
	\label{sec:Q2}
	I first generate all the possible next actions that the agent can do, then use "evaluate\_function" to get the score of current action, then find the highest score, and return the action related to that score\\
	I use recurse to achieve the evaluate\_actions, which needs 3 inputs, agent\_index, depth, game\_state. This single method can act as both min-value and max-value based on current agent index.\\
	The problem of such agent is the speed of get action. In my Mac, for a "smallClassic" layout, when depth is set to 4, it usually takes about 3 seconds to do one action. Besides, as the evaluation function is not so good, often, Pacman will remains stay if both ghosts and food are a little far away from it.
	\section{Alpha-Beta Prune Agent}
	\label{sec:Q3}
	The technique I used in this question is slightly different from that in section~\ref{sec:Q2},
	\begin{enumerate}
		\item \textbf{Evaluate-action function add alpha and beta parameters}. These two new parameters are used to prune those unused leaves.
		\item \textbf{Use three functions to finish this recurse}. This makes the code much easier to read.
	\end{enumerate}
	This agent acts faster than Minimax Agent. If usually takes about 1.5 seconds to determine the action. However, in the same cases of that to Minimax Agent, this also choose to stay.
	\section{Expectimax Agent}
	\label{sec:Q4}
	It is true that usually ghost just random walks, not an adversary who makes optimal decisions and sometimes Minimax agent choose wrong action. That's why we need Expectimax Agent, which will calculate the possibility of every enemy's action and choose the action with highest expect score.\\
	The code of this agent looks much similar of that in section~\ref{sec:Q2}, the only difference is that instead of return min-value, it will return the average value of all actions.\\
	This gives "trappedClassic" about 49.2\% winning rate (compares to 0\% of Minimax Agent)
	\section{Better Evaluation Function}
	\label{sec:Q5}
	I use similar method as in section~\ref{sec:Q1}, based on the distance of Pacman to ghosts and Pacman to the nearest food.\\
	On my computer, in Question5, average score is 1219.9, win rate is 100\% and running time is about 9s, which I think is a considerable good performance.
	\begin{thebibliography}{1}
		\bibitem{winning_strategy} 
		Plambeck, Thane E., and Greg Whitehead. 
		\textit{The Secrets of Notakto: Winning at X-only Tic-Tac-Toe}. 
		arXiv preprint arXiv:1301.1672. 2013 Jan 8.
	\end{thebibliography}
\end{document}